{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This notebook is for the exploration of Linear Regression -- it corresponds to Lecture Handout 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoload setup (you don't need to edit this cell); instructions to: \n",
    "#   i) enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "#  ii) import the module 'lab_1' (which will contain your functions) in an autoreloadable way \n",
    "%aimport lab_1\n",
    "# iii) indicate that we want autoreloading to happen on every evaluation.\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// This cell disables scrollbars on our output, which is handier when we want to do lots of plots.\n",
    "// Don't edit this cell!\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global setup (you don't need to edit this cell): make the modules we want available in the notebook.\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot setup (you don't need to edit this cell): configure the default plot style to be clear & pleasing.\n",
    "plt.style.use('bmh')\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.7\n",
    "\n",
    "# One other thing: disable scientific notation when displaying numpy matrices.  If you prefer scientific notation,\n",
    "# comment out this line.\n",
    "np.set_printoptions(formatter={'float': '{: 0.6f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions for playing with linear regression.\n",
    "# DON'T EDIT THIS CELL!\n",
    "\n",
    "# Generates noisy data from a polynomial model.\n",
    "# Points are in the range -10 .. 10.\n",
    "#\n",
    "# w = polynomial weights (np.array, first element is 0th power)\n",
    "# n = number of data points to generate.\n",
    "# sigma = std. deviation of (zero-mean Gaussian) noise to add.\n",
    "def generate_data(w, n, sigma):\n",
    "    p = w.size-1;\n",
    "    x = (np.random.rand(n,1) - 0.5)*20;  # uniform over range -10 .. 10\n",
    "    X = lab_1.polynomial_design_matrix(x, order=p);\n",
    "    y = np.matmul(X, w) + np.random.randn(n,1)*sigma;\n",
    "    return x,y;\n",
    "\n",
    "# Generates predicted values for the input values 'x', using\n",
    "# the polynomial model 'w'.\n",
    "def predict(x, w):\n",
    "    poly_order = w.size-1;\n",
    "    X = lab_1.polynomial_design_matrix(x, order=poly_order);\n",
    "    y = np.matmul(X,w);\n",
    "    return y;\n",
    "\n",
    "# Plots a polynomial fit:\n",
    "#\n",
    "# x: input values (x-axis locations)\n",
    "# y: output values (y-axis locations corresponding to x)\n",
    "# w: polynomial coefficients: putative fit to evaluate\n",
    "# w0: polynomial coefficients: the true, underlying polynomial\n",
    "def plot_polynomial_fit(x, y, w, w0):\n",
    "    u = np.arange(-10, 10, .2); \n",
    "    v = predict(u, w);\n",
    "    v0 = predict(u, w0);\n",
    "\n",
    "    plt.axis((-10,10,-10,100))\n",
    "    plt.plot(x, y, 'o', label=\"input data\")\n",
    "    plt.plot(u, v, '-', label=\"polynomial fit\")\n",
    "    plt.plot(u, v0, '--', label=\"Ground Truth\")\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: The 'polynomial design matrix' cell\n",
    "#\n",
    "# Your first exercise is to implement the function 'polynomial_design_matrix'\n",
    "# in the module file lab_1.py.  See the instructions there.  Evaluate this cell\n",
    "# to call that function, to check that it's working.\n",
    "#\n",
    "# You should get a matrix (array of arrays) in which each row contains successive powers of each element of the input:\n",
    "#\n",
    "# example: print(lab_1.polynomial_design_matrix(np.array([2,4,3]), 3))\n",
    "#\n",
    "# [[  1.   2.   4.   8.]\n",
    "#  [  1.   4.  16.  64.]\n",
    "#  [  1.   3.   9.  27.]]\n",
    "#\n",
    "print(lab_1.polynomial_design_matrix(np.array([2,4,3]), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'true polynomial' cell (defines the underlying data generation process)\n",
    "#\n",
    "# Now that we have the components required for least-squares regression (in the file lab_1.py), we can experiment\n",
    "# with fitting some data.  We are going to use data generated from a polynomial with added noise.\n",
    "\n",
    "# First, we define the 'true polynomial', which we will use to generate the data.\n",
    "#\n",
    "# Fundamentally, this is just a question of defining a list of powers, so \n",
    "# the list [0.3, 1.3, -3, 0.1] defines the polynomial 0.3 + 1.3 * x - 3 * x^2 + 0.1 * x^3.\n",
    "# To include higher powers, we just add more elements to the list.\n",
    "#\n",
    "# However, there are three bits of data-massage which complicate this slightly:\n",
    "#   i) We use np.array to turn the python list into a Numpy array\n",
    "#  ii) We use a list of list, rather than just a list, so as to make this array a 1xN matrix (row vector)\n",
    "# iii) We use np.transpose to turn the row vector into a column vector, which is what we want for the matrix multiply.\n",
    "true_poly = np.transpose(np.array([[0, -1, 1.3, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'data generation' cell (generates data)\n",
    "#\n",
    "# First, fix the random number generator so that our results are repeatable.  If you comment out this\n",
    "# line, you will see different data every time you run, which might be a bit more interesting.  But when\n",
    "# working on a program, it's frustrating if you see some surprising result that you want to investigate,\n",
    "# which you then can't reproduce because your data is changing randomly each time.  So fixing the generator\n",
    "# in this way is a good practice.\n",
    "np.random.seed(seed=10)  # The value 10 is arbitrary.\n",
    "\n",
    "# Now we can generate the data, specifying how many points to generate and how much noise to add\n",
    "num_data_points = 100\n",
    "noise_std_dev = 2\n",
    "(data_x, data_y) = generate_data(true_poly, num_data_points, noise_std_dev)\n",
    "\n",
    "# We can have a look at the data with 'plt.plot'.\n",
    "# Experiment with:\n",
    "# different underlying true polynomials (edit and re-evaluate the 'true polynomial' cell, then re-evaluate this cell)\n",
    "# different amounts of noise\n",
    "# different numbers of points\n",
    "#\n",
    "plt.plot(data_x, data_y, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'fitting data - least squares' cell\n",
    "#\n",
    "# Here we use the functions from lab_1.py to fit a polynomial to the data we have generated.\n",
    "proposed_order = 3  # Order of polynomial to fit\n",
    "fit_poly = lab_1.LS_poly(data_x, data_y, proposed_order)\n",
    "\n",
    "# Now show the coefficients of the fitted polynomial. We transpose it to a row vector for neater display.\n",
    "print(\"Fit polynomial: \" + str(np.transpose(fit_poly)))   \n",
    "\n",
    "# Plot the fit polynomial against the data (and show the true polynomial too)\n",
    "plot_polynomial_fit(data_x, data_y, fit_poly, true_poly)\n",
    "\n",
    "# Experiment now with different values for 'proposed order' against different true polynomials, \n",
    "# noise levels, & numbers of data points. Re-evaluate this cell each time to view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: The 'importance of regularization' cell\n",
    "#\n",
    "# The exercise is to modify the 'linear_regression' function in 'lab_1.py' to include a Tikhonov regularization term.\n",
    "# See lecture handout 1, slide 35.\n",
    "#\n",
    "# To demonstrate the effect of regularization, we will generate some noisy data and try to model it \n",
    "# using a polynomial of foolishly high order.\n",
    "#\n",
    "# Note that the values used here are tuned to demonstrate the effect; you are encouraged to\n",
    "# change the values and experiment, but not until you've seen the effect work with the provided values!\n",
    "\n",
    "\n",
    "# First we generate the data to fit.  This stuff is explained in the cells above.  Here\n",
    "# of course it is obvious that we are dealing with a data generating process which is \n",
    "# a 2nd-order polynomial, but in a real application we wouldn't know the underlying process.\n",
    "true_poly = np.transpose(np.array([[0, -1, 1.3]]))\n",
    "np.random.seed(seed=10)\n",
    "num_data_points = 10    # Number of points (10 points is not a lot)\n",
    "noise_std_dev = 8       # Amount of noise (8 is moderately high)\n",
    "(data_x, data_y) = generate_data(true_poly, num_data_points, noise_std_dev)\n",
    "\n",
    "# Now we will fit polynomials, one without regularization and one with.  Note that\n",
    "# these will be identical until you've done the exercise (implementing addition of epsilon).\n",
    "proposed_order = 10  # Order of polynomial to fit (10 == crazy high)\n",
    "epsilon = 10         # Amount of regularization (10 == a lot)\n",
    "fit_poly_no_reg = lab_1.LS_poly(data_x, data_y, proposed_order)\n",
    "fit_poly_with_reg = lab_1.LS_poly(data_x, data_y, proposed_order, epsilon)\n",
    "\n",
    "# Display the coefficients of the fitted polynomials.  The regularized polynomial\n",
    "# should have smaller coefficients.\n",
    "print(\"Fit polynomial (no regularization):\\n\" + str(np.transpose(fit_poly_no_reg))) \n",
    "print(\"Fit polynomial (with regularization):\\n\" + str(np.transpose(fit_poly_with_reg))) \n",
    "\n",
    "# Plot the fit polynomials against the data.  The regularized one should be rather more sensible.\n",
    "plot_polynomial_fit(data_x, data_y, fit_poly_no_reg, true_poly)\n",
    "plot_polynomial_fit(data_x, data_y, fit_poly_with_reg, true_poly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'functions for MSE' cell (you don't need to edit this cell)\n",
    "#\n",
    "# This defines functions for evaluating the MSE resulting from a polynomial fit to some data,\n",
    "# for orders 0 to 12.\n",
    "\n",
    "# Returns the MSE of a polynomial fit of order 'order' to data (x, y)\n",
    "def poly_mse(x, y, order):\n",
    "    p = lab_1.LS_poly(x, y, order, 0)\n",
    "    d = predict(x, p)\n",
    "    return lab_1.mean_squared_error(y, d)\n",
    "\n",
    "# Computes the MSE for polynomial fits to data (x,y) for orders 0--12 inclusive.\n",
    "# Returns the MSEs in a numpy row vector (Nx1 matrix)\n",
    "def mse_vs_order(x, y):\n",
    "    K = 12;\n",
    "    mse = np.zeros(shape=(K+1,1))\n",
    "    for p in range(0, K+1):\n",
    "        mse[p] = poly_mse(x, y, p);\n",
    "    return mse\n",
    "\n",
    "# Given data (x,y), plots the curve of (log) MSE vs polynomial order for the range of orders\n",
    "# used in mse_vs_order.\n",
    "def plot_mse_vs_order(x, y):\n",
    "    log_mse = np.log(mse_vs_order(data_x, data_y));\n",
    "    plt.plot(log_mse, '-')\n",
    "    plt.ylabel('log(MSE) for training set')\n",
    "    plt.xlabel('polynomial order')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: The 'Mean-squared-error'cell\n",
    "\n",
    "# Exercise 3 is to implement a function 'mean_squared_error(x, y)' in your\n",
    "# lab_1.py file, which computes the MSE between two vectors (slide 6 of lecture handout 1).\n",
    "\n",
    "# Complete that now, and then evaluate this cell to check:\n",
    "array_1 = np.array([1] * 10)     # A numpy array from the python list containing '1' ten times\n",
    "array_2 = np.array(range(0, 10)) # A numpy array from the python list built from the range 0--9 (inclusive).\n",
    "print(lab_1.mean_squared_error(array_1, array_2))  # This should print 20.5 for the MSE of [1 ten times] vs [0..9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'evaluating the MSE' cell\n",
    "#\n",
    "# Here we will generate some data and examine how the MSE of a polynomial fit to the data, against\n",
    "# the data itself, behaves.  For the initial values in the cell, we should see more-or-less monotonic\n",
    "# reduction of MSE with order.\n",
    "\n",
    "# Generate some data (this code explained in cells above)\n",
    "true_poly = np.transpose(np.array([[0, -1, 1.3, 0]]))\n",
    "num_data_points = 1000    # 1000 is a pretty good amount of data in this context -- experiment with 10, 100, 10000\n",
    "noise_power = 200         # 200 is a lot of noise, even for 1000 points\n",
    "np.random.seed(seed=10)\n",
    "(data_x, data_y) = generate_data(true_poly, num_data_points, noise_power)\n",
    "\n",
    "# Draw the plot\n",
    "plot_mse_vs_order(data_x, data_y)\n",
    "\n",
    "# Experiment with other polynomials, more / less data, more / less noise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'more functions for MSE' cell (you don't need to edit this cell)\n",
    "#\n",
    "# This defines functions for evaluating the MSE resulting from a polynomial fit to some data,\n",
    "# for orders 0 to 12.  Here two sets of data are used:\n",
    "# the 'training set' (training_x, training_y) is used to fit the polynomial, i.e. compute the coefficients\n",
    "# the 'test set' (test_x, test_y) is used to evaluate the fit, i.e. the MSE is computed using these.\n",
    "\n",
    "def poly_mse_train_vs_test(training_x, training_y, test_x, test_y, order):\n",
    "    p = lab_1.LS_poly(training_x, training_y, order, 0)\n",
    "    d = predict(test_x, p)\n",
    "    return lab_1.mean_squared_error(test_y, d)\n",
    "\n",
    "def mse_vs_order_train_vs_test(training_x, traning_y, test_x, test_y):\n",
    "    max_order = 12;\n",
    "    mse = np.zeros(shape=(max_order+1,1)) # '+1' to allow for 0\n",
    "    for p in range(0, max_order+1):       # '+1' as end of range is not included\n",
    "        mse[p] = poly_mse_train_vs_test(training_x, training_y, test_x, test_y, p);\n",
    "    return mse\n",
    "\n",
    "def plot_mse_vs_order_train_vs_test(training_x, training_y, test_x, test_y):\n",
    "    log_mse = np.log(mse_vs_order_train_vs_test(training_x, training_y, test_x, test_y));\n",
    "    plt.plot(log_mse, '-')\n",
    "    plt.ylabel('log(MSE) for training set')\n",
    "    plt.xlabel('polynomial order')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'importance of evaluating on unseen data' cell (Lecture handout 1 slides 28-29)\n",
    "#\n",
    "# Here again we generate some data and look at how the MSE behaves as we model \n",
    "# this data with polynomials of various order.\n",
    "#\n",
    "# However, here we will fit the polynomial to one set of data but evaluate the MSE\n",
    "# using another set of data from the same source.  This is crucial to checking\n",
    "# if the model which we fit to our data will generalize to future observations\n",
    "# of the same process.\n",
    "#\n",
    "# In this lab, 'from the same source' is equivalent to 'identically generated';\n",
    "# I've been a bit lazy in generating the data by using two calls to 'generate_data'.\n",
    "# In the real world we would have one set of data and we would partition it into a training\n",
    "# set and a test set -- or, in a more elaborate scheme, multiple such sets, all to be covered\n",
    "# in future labs.\n",
    "\n",
    "# Again with the generating the data.\n",
    "true_poly = np.transpose(np.array([[0, -1, 1.3, 0]]))\n",
    "noise_power = 200\n",
    "num_training_data_points = 1000\n",
    "num_test_data_points = 1000\n",
    "\n",
    "np.random.seed(seed=10)\n",
    "(training_x, training_y) = generate_data(true_poly, num_training_data_points, noise_power)\n",
    "(test_x, test_y) = generate_data(true_poly, num_test_data_points, noise_power)\n",
    "\n",
    "# Now we plot MSE vs order: first using the same set for fitting & evaluation...\n",
    "plot_mse_vs_order_train_vs_test(training_x, training_y, training_x, training_y)\n",
    "# ... and second using a separate set for evaluation.\n",
    "plot_mse_vs_order_train_vs_test(training_x, training_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: just to confirm your understanding\n",
    "#\n",
    "# Change the function 'question_4' to return the best order for a\n",
    "# polynomial fit to this data:\n",
    "\n",
    "# Load the data\n",
    "data = np.loadtxt('lab_1.data', delimiter=',')\n",
    "\n",
    "# Split it into two equal sets, for training and test.\n",
    "num_points = data.shape[1]\n",
    "set_size = num_points //  2\n",
    "training_x = data[0,0:set_size]\n",
    "training_y = data[1,0:set_size]\n",
    "test_x = data[0,set_size:]\n",
    "test_y = data[1,set_size:]\n",
    "\n",
    "# Now plot MSE vs order: first using the same set for fitting & evaluation...\n",
    "plot_mse_vs_order_train_vs_test(training_x, training_y, training_x, training_y)\n",
    "# ... and second using a separate set for evaluation.\n",
    "plot_mse_vs_order_train_vs_test(training_x, training_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Final Cell: observe how more data protects from overfitting\n",
    "\n",
    "# Our friend the polynomial, old and true.\n",
    "true_poly = np.transpose(np.array([[0, -1, 1.3, 0]]))\n",
    "\n",
    "# Lots of noise:\n",
    "noise_power = 200\n",
    "\n",
    "proposed_order = 12  # Order of polynomial to fit (12 == crazy high)\n",
    "\n",
    "np.random.seed(seed=10)  # Repeatable results\n",
    "\n",
    "for amount_of_data_log_10 in range(1,6):\n",
    "    # Each time through the loop we get 10x the data, so 10; 100; 1000; 10,000; 100,000.\n",
    "    num_data_points = 10**amount_of_data_log_10\n",
    "    (data_x, data_y) = generate_data(true_poly, num_data_points, noise_power)\n",
    "    fit_poly_no_reg = lab_1.LS_poly(data_x, data_y, proposed_order)\n",
    "    plot_polynomial_fit(data_x, data_y, fit_poly_no_reg, true_poly)\n",
    "    print(\"Fit polynomial (no regularization):\\n\" + str(np.transpose(fit_poly_no_reg)))\n",
    "    print(\"\\n\")  # Just to space out the results a bit.\n",
    "    \n",
    "# Notice how with more data the fits become more sensible (less wobbly).  The same effect\n",
    "# can be seen in the numerical coefficients: the higher-power coefficients go towards 0\n",
    "# as the data increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}